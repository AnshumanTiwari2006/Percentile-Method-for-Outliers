ğŸ“‰ Outlier Detection & Removal using the Percentile Method
Flexible, Distribution-Agnostic Outlier Handling for Real-World Data

A practical demonstration of how to identify and manage extreme values using percentile-based thresholds â€” a robust, intuitive approach that works well even when data isnâ€™t normally distributed. This notebook compares two key strategies: removing outliers vs. capping them, so you can choose the best fit for your use case.

âœ… Ideal for income, revenue, or any skewed numerical feature where extreme values are common but not necessarily invalid.

ğŸ“Œ Overview
Using a customer dataset with an income column, this notebook walks through a complete outlier treatment workflow based on the 1st and 99th percentiles. By excluding only the most extreme 2% of values (1% on each tail), it offers a balanced way to clean data without over-trimming.

ğŸ” Key Steps Demonstrated

ğŸ“¥ Data Inspection
The dataset includes demographic and behavioral features like age, gender, days_on_platform, and purchases, with income as the primary focus for outlier analysis. Initial inspection confirms the presence of a long-tailed, right-skewed distribution â€” typical of real-world financial data.

ğŸ“ Percentile-Based Boundary Setting
Instead of relying on assumptions about data shape (like Z-scores) or quartiles (like IQR), this method uses empirical percentiles:

Lower Limit: 1st percentile â†’ 1,077.92
Upper Limit: 99th percentile â†’ 255,072.86
This defines a â€œreasonableâ€ range that excludes only the most extreme observations.

âœ‚ï¸ Trimming (Outlier Removal)
A filtered version of the dataset is created by excluding all records where income falls outside the 1stâ€“99th percentile range.

Result: 100 rows removed (from 5,000 â†’ 4,900)
Use when: Outliers are likely errors or irrelevant to your analysis
ğŸ§¢ Capping (Winsorization)
Instead of deletion, extreme values are pulled back to the nearest boundary:

Incomes below 1,077.92 become 1,077.92
Incomes above 255,072.86 become 255,072.86
Result: Full dataset retained (5,000 rows), but extreme influence neutralized
Use when: Every observation is valuable (e.g., rare high-income customers)
ğŸ“Š Visual Validation
Trimmed data: Histogram shows a smoother, more concentrated distribution
Capped data: Histogram reveals clear peaks at the boundaries â€” visual proof of capping
Box plots: After capping, no outliers appear, confirming successful treatment
These visuals make it easy to see how each method reshapes the data.

ğŸ’¡ Key Takeaways for Practitioners

ğŸ¯ Percentiles are flexible: No assumption of normality needed
âš–ï¸ Trimming vs. Capping: Choose based on data value vs. noise trade-off
ğŸ‘€ Always visualize: Histograms and box plots reveal what summary stats hide
ğŸ”’ Preserve sample size: Capping is often preferred in modeling to avoid data loss
ğŸ“ 1%â€“99% is a common default, but adjust based on domain knowledge (e.g., 5%â€“95% for more aggressive cleaning)
ğŸš€ When to Use This Approach

Working with skewed or heavy-tailed data (e.g., income, sales, latency)
You need a simple, interpretable outlier rule
You want to avoid parametric assumptions
Downstream models are sensitive to extreme values (e.g., linear regression, clustering)
ğŸ“¬ Feedback & Contributions
Want to compare with IQR or Z-score methods, add dynamic percentile selection, or include missing value handling?
ğŸ‘‰ Open an issue or submit a pull request â€” all ideas are welcome! ğŸ¤
